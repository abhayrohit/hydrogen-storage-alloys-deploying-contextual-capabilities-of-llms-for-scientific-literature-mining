# Hydrogen Storage Alloys - LLM Literature Mining

## ğŸš€ Live Demo
**Frontend**: https://arooon-n.github.io/literature-mining-hydrogen-storage-alloys/

**Backend**: You need to run the backend locally (instructions below)

## ğŸ“‹ Prerequisites
- Python 3.8+
- Ollama running locally with `gpt-oss:120b-cloud` model
- Git

## ğŸ› ï¸ Setup & Run

### 1. Clone the repository
```bash
git clone https://github.com/arooon-n/literature-mining-hydrogen-storage-alloys.git
cd literature-mining-hydrogen-storage-alloys
```

### 2. Install Python dependencies
```bash
python -m pip install -r requirements.txt
```

### 3. Start the backend server
```bash
python main_fastapi.py
```

The backend will start at: http://localhost:8000

### 4. Access the application
**Option A - Use GitHub Pages (Recommended)**
- Open: https://arooon-n.github.io/literature-mining-hydrogen-storage-alloys/
- This will connect to your local backend at http://localhost:8000

**Option B - Use Local Frontend**
- Open: http://localhost:8000/

## ğŸŒ GitHub Pages Deployment

The frontend is automatically deployed to GitHub Pages when you push to the `main` branch.

### Manual deployment (if needed):
```bash
# Push your changes
git add .
git commit -m "Update frontend"
git push origin main
```

GitHub Actions will automatically:
1. Build and deploy the frontend to GitHub Pages
2. Configure it to connect to http://localhost:8000 for API calls

## ğŸ“ Configuration

### Environment Variables (Optional)
```bash
# Windows PowerShell
$env:OLLAMA_URL="http://localhost:11434"
$env:MODEL_NAME="gpt-oss:120b-cloud"

# Linux/Mac
export OLLAMA_URL="http://localhost:11434"
export MODEL_NAME="gpt-oss:120b-cloud"
```

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ main_fastapi.py          # Backend API server
â”œâ”€â”€ llm_extractor.py          # LLM extraction logic
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ frontend/                 # Frontend files (deployed to GitHub Pages)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ data/                     # Data directory
â”‚   â”œâ”€â”€ pdfs/
â”‚   â””â”€â”€ raw_text/
â”œâ”€â”€ outputs/                  # Extracted CSV files
â””â”€â”€ uploads/                  # Uploaded PDFs

```

## ğŸ”§ Troubleshooting

### Backend not reachable
- Ensure the backend is running: `python main_fastapi.py`
- Check firewall settings allow localhost:8000
- Verify Ollama is running: `ollama list`

### CORS errors
- The backend is configured to allow all origins
- Clear browser cache (Ctrl+Shift+R)

### GitHub Pages not updating
- Check Actions tab in GitHub repository
- Ensure GitHub Pages is enabled in repository settings
- Wait 1-2 minutes after push for deployment

## ğŸ“Š Features
- PDF upload and text extraction
- Multi-chunk LLM processing for large papers
- Alloy data extraction with AI model
- CSV export of extracted data
- Real-time progress tracking
- Loading overlays during processing

## ğŸ‘¥ Team
AIE - B | Group - 17

## ğŸ“„ License
MIT License
